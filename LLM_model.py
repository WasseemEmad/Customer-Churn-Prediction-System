from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint
import os
import re
import ast


API_KEY = ""
MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"

os.environ['HUGGINGFACEHUB_API_TOKEN'] = API_KEY


def LLM_model(MODEL_ID, temperature=0.7):
    """
    Initializes and returns an LLM model using the Hugging Face endpoint.

    Args:
    - MODEL_ID (str): The model identifier from Hugging Face.
    - temperature (float, optional): Controls randomness in text generation (default: 0.7).

    Returns:
    - HuggingFaceEndpoint: An instance of the LLM model.
    """
    llm = HuggingFaceEndpoint(
        repo_id=MODEL_ID,
        temperature=temperature,
        task="text-generation"  
        )
    return llm


def generate_customer_info(llm,content):
    """
    Uses the LLM to extract structured customer information from a given text.

    Args:
    - llm (HuggingFaceEndpoint): The LLM model instance.
    - content (str): The raw customer description provided by the user.

    Returns:
    - str: The LLM-generated dictionary containing extracted customer details.
    """
    
    prompt = f'''
    **Task:** Extract and return customer information in a specific order as a dictionary.
    Only return dictionary not add any information or notes or anything.

    **Instructions:**
    1. **Extract the following features from the provided query in the exact order listed below.**
    2. **For each feature, select the value from the given options only.**
    3. **Return the values as a Python dictionary in the specified order. Ensure the values are case-sensitive and match exactly as described in the options.**
    4. **The dictionary keys are the features.**

    **Order of Features and Allowed Values:**
    1. **gender**: 'Male' or 'Female'
    2. **senior_citizen**: 1 (for Yes) or 0 (for No)
    3. **is_married**: 'Yes' or 'No'
    4. **dependents**: 'Yes' if the person has children or 'No' if the person has no children
    5. **tenure**: A number representing the number of months
    6. **phone_service**: 'Yes' or 'No'
    7. **dual**: 'Yes' or 'No' or 'No phone service'
    8. **internet_service**: 'DSL','Fiber optic', or 'No'
    9. **online_security**: 'Yes','No', or 'No internet service'
    10. **online_backup**: 'Yes', 'No', or 'No internet service'
    11. **device_protection**: 'Yes', 'No', or 'No internet service'
    12. **tech_support**: 'Yes', 'No', or 'No internet service'
    13. **streaming_tv**: 'Yes', 'No', or 'No internet service'
    14. **streaming_movies**: 'Yes', 'No', or 'No internet service'
    15. **contract**: 'Month-to-month', 'One year', or 'Two year'
    16. **paperless_billing**: 'Yes' or 'No'
    17. **payment_method**: 'Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'
    18. **monthly_charges**: A number representing the monthly charges
    19. **total_charges**: A number representing the total charges

    Important:
    -The dictionary must strictly follow the order mentioned.
    -Do not return any additional text or explanation, only the dictionary.

    Customer information:

    {content}
    '''

    llm = llm

    response = llm.invoke(prompt)
    
    return response

def clean_llm_data(response):
    """
    Extracts a dictionary from the LLM response and converts it into a Python dictionary.

    Args:
    - response (str): The raw response from the LLM.

    Returns:
    - dict: Extracted customer information as a structured dictionary.
    """
    match = re.search(r"\{.*\}", response, re.DOTALL)
    if match:
        dictionary_str = match.group(0)  


        customer_info = ast.literal_eval(dictionary_str)
        print(type(customer_info))  
        print(customer_info)  
    else:
        print("No dictionary found in the output")
        
    return customer_info

def extract_analysis(llm_response):
    """
    Extracts only the analysis text from the LLM response by removing '**Response:**'.
    
    Args:
    - llm_response (str): The full response generated by the LLM.
    
    Returns:
    - str: Cleaned analysis text.
    """

    cleaned_response = re.sub(r"^\*\*Response:\*\*\s*", "", llm_response.strip(), flags=re.MULTILINE)
    
    return cleaned_response


def generate_analysis(llm, customer_data, churn_probability):
    """
    Generates a brief analysis of the customer's profile and their likelihood of churning.

    Args:
    - llm (HuggingFaceEndpoint): The LLM model instance.
    - customer_data (dict): Extracted structured data about the customer.
    - churn_probability (float): The predicted probability of churn.

    Returns:
    - str: A short paragraph explaining the churn prediction.
    """
    prompt = f'''
    **Task:** Provide a brief analysis of the customer's profile and their likelihood of churning.

    **Customer Profile:** {customer_data}

    **Churn Probability:** {churn_probability:.2f}

    **Instructions:**
    - Analyze the customer's profile and explain why they might stay or leave.
    - Highlight key factors that contribute to their churn risk.
    - Keep the response concise (2-3 sentences).
    
    **Output Format:**
    - Return a short paragraph without extra text or disclaimers.
    '''

    response = llm.invoke(prompt)

    tenure_months = customer_data.get('tenure', 0)

    if tenure_months < 12:
        response = response.replace(" year", " month").replace(" years", " months")
    cleaned_response = extract_analysis(response)
    
    if re.match(r'^[\d\s]+$', response.strip()) or not re.search(r'[a-zA-Z]', response.strip()):
        
        return "No meaningful analysis could be generated."
    
    return cleaned_response


